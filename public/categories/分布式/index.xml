<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>分布式 on Messiz Blog</title>
        <link>http://localhost:1313/categories/%E5%88%86%E5%B8%83%E5%BC%8F/</link>
        <description>Recent content in 分布式 on Messiz Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Messiz</copyright>
        <lastBuildDate>Wed, 18 Sep 2024 02:15:02 +0800</lastBuildDate><atom:link href="http://localhost:1313/categories/%E5%88%86%E5%B8%83%E5%BC%8F/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Lab-3 Raft</title>
        <link>http://localhost:1313/p/lab-3-raft/</link>
        <pubDate>Wed, 18 Sep 2024 02:15:02 +0800</pubDate>
        
        <guid>http://localhost:1313/p/lab-3-raft/</guid>
        <description>&lt;h1 id=&#34;raft&#34;&gt;Raft
&lt;/h1&gt;&lt;h2 id=&#34;脑裂&#34;&gt;脑裂
&lt;/h2&gt;&lt;p&gt;对于MapReduce、GFS等分布式系统，都是一个Master节点与很多副节点组成的。这种单主结构足够简单，不会出现矛盾（毕竟master自己不会反对自己），但也带来了问题——一旦这个节点崩溃，那么整个系统不能正常工作。&lt;/p&gt;
&lt;p&gt;以Test-and-Set为例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240917212630.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;实际上是单点的，我们为了提高系统的容错性，设计一个多副本的Test-and-Set服务（S1和S2）。&lt;/p&gt;
&lt;p&gt;现在，我们来假设我们有一个网络，这个网络里面有两个服务器（S1，S2），这两个服务器都是我们Test-and-Set服务的拷贝。这个网络里面还有两个客户端（C1，C2），它们需要通过Test-and-Set服务确定主节点是谁。在这个例子中，这两个客户端本身就是VMware FT中的Primary和Backup虚拟机。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果我们规定client需要与两个server同时交互，那么会比单点更糟糕，因为一旦一个server崩溃，则不能正常服务，导致有副本系统反而没有容错性；&lt;/li&gt;
&lt;li&gt;如果我们规定只与其中一个server交互即可，则会发生脑裂。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当时（1980s）解决脑裂大致两种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构建不会出错的“网络”&lt;/li&gt;
&lt;li&gt;人工解决问题，出现问题时，停止提供服务，由人工检查问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;过半票决majority-vote&#34;&gt;过半票决（Majority Vote）
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;完成任何操作，都需要过半的服务器（总服务器数的一半以上）批准来完成对应操作。&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务器总数应该为&lt;strong&gt;奇数&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;假设系统有2*F+1个服务器，那么系统最多可接受F个服务器故障（F+1个服务器正常运行），仍然可以正常工作；&lt;/li&gt;
&lt;li&gt;过半票决的一个特性是：&lt;strong&gt;每一个操作对应的过半服务器，必然至少包含一个服务器存在于上一个操作的过半服务器中&lt;/strong&gt;。下面的例子说明了这一点：
&lt;ul&gt;
&lt;li&gt;过半票决系统的一个特性就是，最多只有一个网络分区会有过半的服务器，所以我们不可能有两个分区可以同时完成操作。这里背后更微妙的点在于，如果你总是需要过半的服务器才能完成任何操作，同时你有一系列的操作需要完成，其中的每一个操作都需要过半的服务器来批准，例如选举Raft的Leader，那么&lt;strong&gt;每一个操作对应的过半服务器，必然至少包含一个服务器存在于上一个操作的过半服务器中。也就是说，任意两组过半服务器，至少有一个服务器是重叠的。&lt;strong&gt;实际上，相比其他特性，Raft更依赖这个特性来避免脑裂。例如，当一个Raft Leader竞选成功，那么这个Leader必然凑够了&lt;/strong&gt;过半服务器的选票&lt;/strong&gt;，而这组过半服务器中，&lt;strong&gt;必然与旧Leader的过半服务器有重叠&lt;/strong&gt;。所以，新的Leader必然知道旧Leader使用的任期号（term number），因为新Leader的过半服务器必然与旧Leader的过半服务器有重叠，而旧Leader的过半服务器中的每一个&lt;strong&gt;必然都知道旧Leader的任期号&lt;/strong&gt;。类似的，任何旧Leader提交的操作，必然存在于过半的Raft服务器中，而任何新Leader的过半服务器中，必然有至少一个服务器包含了旧Leader的所有操作。这是Raft能正确运行的一个重要因素。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;raft初探&#34;&gt;Raft初探
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240917212635.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Raft作为Library运行在服务中，为服务提供一致性保证。如上图，一个服务的副本由两部分组成：上半部分是&lt;strong&gt;应用程序（APP）（上图中是一个KV Server）&lt;/strong&gt;，下半部分是&lt;strong&gt;Raft库&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;执行流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Client发送请求至Leader节点的APP；&lt;/li&gt;
&lt;li&gt;APP将Client请求向下发送至Raft层，Raft层将操作存入日志；&lt;/li&gt;
&lt;li&gt;Raft将此操作发送值所有副本；&lt;/li&gt;
&lt;li&gt;当Leader得知过半的阶段都有了这个操作的拷贝后，发送消息至上层APP，告知APP可以实际执行这个操作了；此后所有其他副本都将操作提交至本地的APP执行，理想情况下，整个系统的所有副本都执行了此操作，APP与Raft状态应都保持一致。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;log同步时序&#34;&gt;Log同步时序
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240917212638.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在系统正常工作没有故障时，Raft Log 同步流程的时序图如上：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Client发送请求；&lt;/li&gt;
&lt;li&gt;Leader向其余所有副本同步消息（&lt;strong&gt;AppendEntries RPC&lt;/strong&gt;，用于日志同步与心跳信号传递）；&lt;/li&gt;
&lt;li&gt;待收到超半数的确认后提交（对于上面三个副本的系统，包括Leader自身只需要额外收到一个节点的确认即可）；&lt;/li&gt;
&lt;li&gt;Leader将提交的消息同步给其他副本，通知它们可以提交操作（在Raft中，没有单独的提交信息，commit消息是携带在下一次的AppendEntries RPC中的）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;日志raft-log&#34;&gt;日志（Raft Log）
&lt;/h2&gt;&lt;p&gt;日志的作用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leader用来对&lt;strong&gt;操作进行排序&lt;/strong&gt;，这对于复制状态机十分重要；&lt;/li&gt;
&lt;li&gt;Follower用于&lt;strong&gt;临时存储操作&lt;/strong&gt;，待收到Leader的commit消息执行操作；&lt;/li&gt;
&lt;li&gt;Leader中存放操作，如果因为网络或其他问题导致副本没有收到消息，可以进行&lt;strong&gt;消息重传&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;所有节点都会将Log持久化，这有助于节点&lt;strong&gt;故障后恢复状&lt;/strong&gt;态。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;leader选举leader-election&#34;&gt;Leader选举（Leader Election）
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Raft中设置Leader的好处在于，对于一个请求，存在Leader的情况下，只需一轮交互即可取得过半服务器的确认，而对于无Leader系统，通常需要一轮消息确认一个临时Leader，第二轮消息才能实际确认请求，性能更高；&lt;/li&gt;
&lt;li&gt;在Raft生命周期中，通过**任期号（term number）**来区分不同的Leader，每个任期内最多只有一个Leader（可能没有或者有一个）；&lt;/li&gt;
&lt;li&gt;每个Raft节点都有一个&lt;strong&gt;选举定时器（Election Timer）&lt;/strong&gt;，当此定时器时间耗尽之前，没有收到任何来自Leader的消息，则认为Leader已经下线，并开始一次新的选举；
&lt;ul&gt;
&lt;li&gt;任何一条AppendEntries消息的到达都会重置选举定时器，因此，在网络正常且节点无故障的情况下，不会进行新的选举；&lt;/li&gt;
&lt;li&gt;选举失败可能的情况：
&lt;ul&gt;
&lt;li&gt;可投票节点不足（网络故障或服务器关机等）&lt;/li&gt;
&lt;li&gt;分割选票（Split Vote）：如果若干节点选举定时器同时到期，同时都发出了RequestVote消息，这造成所有这些节点都无法获得足够的选票，就会导致这次选举失败。Raft不能完全避免这种情况发生，但是可以大大降低发生概率：&lt;strong&gt;为选举定时器随机选择超时时间，以降低多个节点同时超时的发生概率。&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;超时时间选择：&lt;strong&gt;下限&lt;/strong&gt;不能低于一个心跳间隔，否则会一直超时，通常可以设置为2-3个心跳时间；&lt;strong&gt;上限&lt;/strong&gt;选择取决于期望的故障恢复速度，同时要保证随机的超时时间间隔足够完成选举。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;开始一次选举时，当前服务器会&lt;strong&gt;增加任期号（term number）&lt;/strong&gt;，并发出&lt;strong&gt;请求投票（RequestVote）RPC&lt;/strong&gt;，这个消息会发送至其他所有节点（因为自己总会投票给自己）；&lt;/li&gt;
&lt;li&gt;相同的，选举视为一次操作的话，也需要获得超半数的同意票才能当选。每个Raft节点在一个任期内只会进行一次投票，保证了一次选举最多只会当选一个Leader；&lt;/li&gt;
&lt;li&gt;当选的Leader会立刻发送AppendEntries告知其他所有节点，当然不是直接写“我当选了Leader！”，而是附带上当前任期号，而由于Raft规定，对于某个任期，只有Leader才能发出AppendEntries消息。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;系统故障下的机制&#34;&gt;系统故障下的机制
&lt;/h2&gt;&lt;h3 id=&#34;日志恢复log-backup&#34;&gt;日志恢复（Log Backup）
&lt;/h3&gt;&lt;p&gt;当Raft系统发生故障时，可能系统中不同节点中保存的log不一致，下面举例说明：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;服务器&lt;/th&gt;
&lt;th&gt;Slot 10&lt;/th&gt;
&lt;th&gt;Slot 11&lt;/th&gt;
&lt;th&gt;Slot 12&lt;/th&gt;
&lt;th&gt;Slot 13&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;S1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;&lt;font style=&#34;color:#DF2A3F;&#34;&gt;6&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol&gt;
&lt;li&gt;有S1、S2、S3三个服务器，在任期3中，第一次请求三者都成功记录，第二次请求只有S2、S3记录，S1记录失败，包含三种情况（假设S3是任期3的Leader）：
&lt;ul&gt;
&lt;li&gt;发送给S1的消息丢了&lt;/li&gt;
&lt;li&gt;S1当时已经关机了&lt;/li&gt;
&lt;li&gt;S3在向S2发送完AppendEntries之后，在向S1发送AppendEntries之前故障了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;继续往后看：
&lt;ul&gt;
&lt;li&gt;S2称为Leader后接受一个请求后崩溃，需要进行重新选举&lt;/li&gt;
&lt;li&gt;S3成功当选，获得请求，并且崩溃&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;假设下一个任期是6，并且S3当选Leader，S3会发送任期6的第一个AppendEntries RPC给其他两个follower，其中不仅包括本条log相关信息，还包括：
&lt;ul&gt;
&lt;li&gt;prevLogIndex 前一项log的槽位&lt;/li&gt;
&lt;li&gt;prevLogTerm 前一项log的Term number&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;S2检查是否匹配，结果发现自己前一项Log的任期为4而非5，因此会拒绝这个AppendEntries RPC，并返回False给Leader S3，S1前一项没有值，因此也将拒绝这个AE RPC。&lt;/li&gt;
&lt;li&gt;Leader中还维护了Follower的nextIndex，nextIndex的初始值为新当选的Leader的最后一条日志位置，在上例中就是13。而由于Leader收到两个Follower的False，则会减小对应的nextIndex，此时nextIndex=12，prevLogIndex=11，prevLogTerm=3，此时发送的AE消息包含了prevLogIndex后的所有条目（也就是12，13两个槽的内容）。&lt;/li&gt;
&lt;li&gt;此时S2发现prevLogIndex和prevLogTerm与自己记录相符，则接受此AE，并将后续的数据写入自己Log的相同位置（删除了Slot 12中的4，这是安全的因为4没有被commit）。&lt;/li&gt;
&lt;li&gt;S1此时仍不符合，因此还是拒绝。&lt;/li&gt;
&lt;li&gt;Leader重复前面的流程，直至S1也确认AE，并将正确Log写入。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;快速恢复fast-backup&#34;&gt;快速恢复（Fast Backup）
&lt;/h3&gt;&lt;p&gt;如果Follower进度和Leader差太多（例如：差1000个任期），此时进行日志恢复就会造成巨大的时间开销（需要发送1000条AE）。&lt;/p&gt;
&lt;p&gt;一种快速恢复策略如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在新Leader发送第一个AE后，每个Follower回复都附带上一些额外信息：
&lt;ul&gt;
&lt;li&gt;XTerm：Follower中与Leader冲突的Log对应的任期号。在之前有介绍Leader会在prevLogTerm中带上本地Log记录中，前一条Log的任期号。如果Follower在对应位置的任期号不匹配，它会拒绝Leader的AppendEntries消息，并将自己的任期号放在XTerm中。如果Follower在对应位置没有Log，那么这里会返回 -1。&lt;/li&gt;
&lt;li&gt;XIndex：这个是Follower中，对应任期号为XTerm的第一条Log条目的槽位号。&lt;/li&gt;
&lt;li&gt;XLen：如果Follower在对应位置没有Log，那么XTerm会返回-1，XLen表示空白的Log槽位数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面假设三个场景&lt;br&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240917212648.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Case 1：Follower（S1）会返回XTerm=5，XIndex=2。Leader（S2）发现自己没有任期5的日志，它会将自己本地记录的，S1的nextIndex设置到XIndex，也就是S1中，任期5的第一条Log对应的槽位号。所以，如果Leader完全没有XTerm的任何Log，那么它应该回退到XIndex对应的位置（这样，Leader发出的下一条AppendEntries就可以一次覆盖S1中所有XTerm对应的Log）。&lt;/li&gt;
&lt;li&gt;Case 2。Follower（S1）会返回XTerm=4，XIndex=1。Leader（S2）发现自己其实有任期4的日志，它会将自己本地记录的S1的nextIndex设置到本地在XTerm位置的Log条目后面，也就是槽位2。下一次Leader发出下一条AppendEntries时，就可以一次覆盖S1中槽位2和槽位3对应的Log。&lt;/li&gt;
&lt;li&gt;Case 3。Follower（S1）会返回XTerm=-1，XLen=2。这表示S1中日志太短了，以至于在冲突的位置没有Log条目，Leader应该回退到Follower最后一条Log条目的下一条，也就是槽位2，并从这开始发送AppendEntries消息。槽位2可以从XLen中的数值计算得到。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;选举约束election-restriction&#34;&gt;选举约束（election Restriction）
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;候选人最后一条Log条目的任期号&lt;strong&gt;大于&lt;/strong&gt;本地最后一条Log条目的任期号；&lt;/li&gt;
&lt;li&gt;或者，候选人最后一条Log条目的任期号&lt;strong&gt;等于&lt;/strong&gt;本地最后一条Log条目的任期号，且候选人的Log记录长度&lt;strong&gt;大于等于&lt;/strong&gt;本地Log记录的长度&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;持久化persistence&#34;&gt;持久化（Persistence）
&lt;/h3&gt;&lt;p&gt;Raft持久化存储三种数据：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Log：Log记录APP的所有操作，Raft并没有规定持久化APP的运行状态，因此，Log是唯一保存APP状态的数据，当从故障恢复时，可以从Log逐条读取数据恢复APP状态；&lt;/li&gt;
&lt;li&gt;currentTerm：与voteFor保证Leader选举的正确性，新的Leader需要知道正确的Term number，假设前一个Leader故障，而剩下的两个Follower缺失一些Log，导致不知道正确Term，那么重新选举会造成新Leader存储错误Term number；&lt;/li&gt;
&lt;li&gt;voteFor：不存储voteFor会造成重复投票，造成脑裂。S1为S2投完票后故障，恢复后又收到S3的投票请求，而认为自己没有投过票，又给S3投票，可能造成两个Leader的情况发生。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;日志快照log-snapshot&#34;&gt;日志快照（Log Snapshot）
&lt;/h3&gt;&lt;p&gt;Raft系统长时间运行会造成Log内容过多的情况，这对于持久化以及故障后恢复的效率会产生较大影响。因此，提出快照（Snapshots）。&lt;/p&gt;
&lt;p&gt;当Raft判断Log大小达到某个阈值时，会要求程序在Log的特定位置上，对其状态做一个快照，然后Raft丢弃这一点之前的Log。&lt;/p&gt;
&lt;p&gt;可能出现的问题：当Follower落后太多，以至于Leader产生快照丢弃的Log无法恢复时，Leader和Follower的一致性会遭到破坏。&lt;/p&gt;
&lt;p&gt;Raft的解决方案如下：&lt;/p&gt;
&lt;p&gt;引入一个新的消息类型，InstallSnapshot RPC&lt;/p&gt;
&lt;p&gt;当Follower刚刚恢复，如果它的Log短于Leader通过 AppendEntries RPC发给它的内容，那么它首先会强制Leader回退自己的Log。在某个点，Leader将不能再回退，因为它已经到了自己Log的起点。这时，Leader会将自己的快照发给Follower，之后立即通过AppendEntries将后面的Log发给Follower。&lt;/p&gt;
&lt;h3 id=&#34;线性一致linearizability&#34;&gt;线性一致（Linearizability）
&lt;/h3&gt;&lt;p&gt;线性一致或者说强一致，定义了一个分布式系统正确的处理逻辑，在客户端看来，就和与单点服务交互一样。&lt;/p&gt;
&lt;p&gt;**定义：一个系统是线性一致的，**&lt;strong&gt;如果执行历史整体可以按照一个顺序排列，且排列顺序与客户端请求的实际时间相符合；并且，每一个读操作都看到的是最近一次写入的值。&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Lab-1 MapReduce</title>
        <link>http://localhost:1313/p/lab-1-mapreduce/</link>
        <pubDate>Tue, 17 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/lab-1-mapreduce/</guid>
        <description>&lt;img src="http://localhost:1313/p/lab-1-mapreduce/matt-le-SJSpo9hQf7s-unsplash.jpg" alt="Featured image of post Lab-1 MapReduce" /&gt;&lt;h1 id=&#34;mapreduce-simplified-data-processing-on-large-cluster&#34;&gt;MapReduce: Simplified Data Processing on Large Cluster
&lt;/h1&gt;&lt;h2 id=&#34;模型&#34;&gt;模型
&lt;/h2&gt;&lt;p&gt;MapReduce由Map+Reduce两部分构成&lt;/p&gt;
&lt;h3 id=&#34;map&#34;&gt;Map
&lt;/h3&gt;&lt;p&gt;将输入的pair转化为中间的k/v对&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924001534.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;reduce&#34;&gt;Reduce
&lt;/h3&gt;&lt;p&gt;将中间的k/v对转化为最终的输出&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924001544.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;例子&#34;&gt;例子
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924001552.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;实现&#34;&gt;实现
&lt;/h2&gt;&lt;h3 id=&#34;流程&#34;&gt;流程
&lt;/h3&gt;&lt;p&gt;!&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924001601.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将输入文件分为M个小块，然后在集群的所有机器上启动程序&lt;/li&gt;
&lt;li&gt;用户程序分为master和worker
&lt;ol&gt;
&lt;li&gt;master负责分发任务&lt;/li&gt;
&lt;li&gt;worker负责处理具体工作，例如一个map任务的worker负责从输入数据中提取k/v对，并使用Map函数处理，得到中间k/v对，存储在内存缓冲区&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;内存缓冲区中的数据周期性地写入磁盘，并且由分区函数分为R个partition，并将结果交给master，master可以将这些位置信息交给负责reduce任务的worker&lt;/li&gt;
&lt;li&gt;reduce worker接到磁盘上partition的位置信息后，使用RPC来读取这些数据，然后按照key对数据进行排序&lt;/li&gt;
&lt;li&gt;reduce worker然后遍历已排序的数据，将每一个不同的key以及其对应的value集合传递给Reduce函数进行处理，并输出此分区最终的输出文件&lt;/li&gt;
&lt;li&gt;当所有的map和reduce函数完成工作后，master唤醒用户程序，返回数据&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;master&#34;&gt;Master
&lt;/h3&gt;&lt;p&gt;master负责调度worker，因此对于每一个map task和reduce task，都保存了其状态信息（空闲、正在处理、处理完毕），以及对于正在处理或处理完的任务，保存其工作机器的信息&lt;/p&gt;
&lt;h3 id=&#34;容错&#34;&gt;容错
&lt;/h3&gt;&lt;p&gt;由于MapReduce本身设计基于大的机器集群，因此机器故障一定会发生&lt;/p&gt;
&lt;h4 id=&#34;worker-failure&#34;&gt;Worker Failure
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;master定期ping各个worker，如果没有回应则认为该worker挂掉，对于出问题的worker，其&lt;font style=&#34;color:#DF2A3F;&#34;&gt;完成的所有map任务&lt;/font&gt;以及&lt;font style=&#34;color:#DF2A3F;&#34;&gt;正在进行的map或reduce任务&lt;/font&gt;，都将被标记为idle状态（未完成）以供重新调度。&lt;/li&gt;
&lt;li&gt;之所以完成的map任务需要重新执行，是因为由于该worker联系不上，说明对应的机器也联系不上，而完成的map任务，中间结果是保存在本机的磁盘的，因此需要重新调度执行；而reduce任务不需要是因为其输出是存储于全局的文件系统中。&lt;/li&gt;
&lt;li&gt;当A执行map失败而由B重新执行时，所有后续执行reduce的worker都会接到信息，尚未从A读取数据的worker会转而从B读取数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;master-failure&#34;&gt;Master Failure
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;通过周期性记录checkpoint，当master宕机后，可以从最近的checkpoint恢复&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;semantics-in-the-presence-of-failures&#34;&gt;Semantics in the Presence of Failures
&lt;/h4&gt;&lt;p&gt;当map和reduce函数都是确定性函数时，提供与整个程序无故障顺序执行相同的输出。&lt;/p&gt;
&lt;p&gt;当map和reduce函数为非确定性函数时，提供较弱但仍然合理的语义。&lt;/p&gt;
&lt;p&gt;&lt;font style=&#34;color:rgb(5, 7, 59);background-color:rgb(253, 253, 254);&#34;&gt;&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&#34;font-stylecolorrgb5-7-59background-colorrgb253-253-254后备任务font&#34;&gt;&lt;font style=&#34;color:rgb(5, 7, 59);background-color:rgb(253, 253, 254);&#34;&gt;后备任务&lt;/font&gt;
&lt;/h3&gt;&lt;p&gt;当出现拖后腿的节点时（由于某些原因导致执行慢），会造成MapReduce整体变慢。&lt;/p&gt;
&lt;p&gt;当MapReduce操作接近完成时，master会调度空闲节点执行当前剩余的正在执行的任务，当任意节点完成了该任务后，就将其标记为完成。&lt;/p&gt;
&lt;h2 id=&#34;改进&#34;&gt;改进
&lt;/h2&gt;&lt;h3 id=&#34;分区函数-partitioning-function&#34;&gt;分区函数 Partitioning Function
&lt;/h3&gt;&lt;p&gt;常见的分区函数：hash(key) mod R，哈希后取模&lt;/p&gt;
&lt;p&gt;对于某些特殊目标，也可以定制特殊的分区函数&lt;/p&gt;
&lt;h3 id=&#34;排序保证-ordering-guarantees&#34;&gt;排序保证 Ordering Guarantees
&lt;/h3&gt;&lt;p&gt;对于一个给定的分区，其中键值对按照key按顺序排列&lt;/p&gt;
&lt;h3 id=&#34;组合函数-combiner-function&#34;&gt;组合函数 Combiner Function
&lt;/h3&gt;&lt;p&gt;某些情况下，map操作产生的中间键存在大量重复，例如对于统计词频的任务，map操作会产生若干&amp;lt;key, 1&amp;gt;的键值对，我们可以引入组合函数，在将其发送至reduce函数前先进行合并，得到&amp;lt;key, n&amp;gt;这样的中间产物。注意，这里的组合函数的实现实际上与reduce函数一致，只是他们的输出不同，组合函数输出仍是中间产物，而reduce函数输出为最终结果。&lt;/p&gt;
&lt;h3 id=&#34;输入输出类型-input-and-output-type&#34;&gt;输入输出类型 Input and Output Type
&lt;/h3&gt;&lt;p&gt;例如对于文本数据，输入kv对，key表示offset，value则为对应行的数据&lt;/p&gt;
&lt;p&gt;用户可以自己定义一些输入输出格式&lt;/p&gt;
&lt;h3 id=&#34;副产物-side-effects&#34;&gt;副产物 Side-effects
&lt;/h3&gt;&lt;p&gt;&lt;font style=&#34;color:#DF2A3F;&#34;&gt;没太懂&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;font style=&#34;color:#DF2A3F;&#34;&gt;&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&#34;跳过错误记录-skipping-bad-records&#34;&gt;跳过错误记录 Skipping Bad Records
&lt;/h3&gt;&lt;p&gt;跳过错误记录从而避免程序崩溃，能够跳过继续执行&lt;/p&gt;
&lt;h3 id=&#34;本地执行-local-execution&#34;&gt;本地执行 Local Execution
&lt;/h3&gt;&lt;p&gt;MapReduce工作在分布式系统中，难以调试，提供了简单的单体执行机制，方便用户调试&lt;/p&gt;
&lt;h3 id=&#34;状态信息-status-information&#34;&gt;状态信息 Status Information
&lt;/h3&gt;&lt;p&gt;Master节点提供任务整体进度信息&lt;/p&gt;
&lt;h3 id=&#34;计数器-counters&#34;&gt;计数器 Counters
&lt;/h3&gt;&lt;p&gt;提供了计数器，各worker节点统计一些计数数据，将数据汇总给master节点&lt;/p&gt;
&lt;h2 id=&#34;性能&#34;&gt;性能
&lt;/h2&gt;&lt;p&gt;两个任务上进行了测试&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;1tb数据排序&lt;/li&gt;
&lt;li&gt;1tb数据搜索&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>分布式事务</title>
        <link>http://localhost:1313/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</link>
        <pubDate>Thu, 13 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</guid>
        <description>&lt;h1 id=&#34;2pc&#34;&gt;2PC
&lt;/h1&gt;&lt;p&gt;Two-phase commit protocal，两阶段提交协议&lt;/p&gt;
&lt;p&gt;从角色来划分，包括一个&lt;strong&gt;协调者（coordinator）&lt;strong&gt;和若干&lt;/strong&gt;参与者（participant）&lt;/strong&gt;；&lt;/p&gt;
&lt;p&gt;从阶段来划分，包括**准备（prepare）&lt;strong&gt;和&lt;/strong&gt;提交（commit）**阶段；&lt;/p&gt;
&lt;p&gt;示意图如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;准备阶段&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924232311.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;提交阶段&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924232315.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;优点&#34;&gt;优点
&lt;/h2&gt;&lt;p&gt;对于业务没有侵入，可以利用数据库自身的功能进行本地事务提交和回滚，而不用额外实现。&lt;/p&gt;
&lt;h2 id=&#34;缺点&#34;&gt;缺点
&lt;/h2&gt;&lt;h3 id=&#34;同步阻塞&#34;&gt;同步阻塞
&lt;/h3&gt;&lt;p&gt;在第一阶段，参与者执行了准备命令后，会处于等待协调者消息的同步阻塞状态，影响处理效率；&lt;/p&gt;
&lt;h3 id=&#34;单点故障&#34;&gt;单点故障
&lt;/h3&gt;&lt;p&gt;其实是一主多从的结构，因此如果主节点（协调者）宕机，则整个事务都不能执行；&lt;/p&gt;
&lt;p&gt;而且如果在协调者发送准备信息后宕机，则所有参与者都将处于同步阻塞状态。&lt;/p&gt;
&lt;h3 id=&#34;数据不一致&#34;&gt;数据不一致
&lt;/h3&gt;&lt;p&gt;可能由于网络问题导致某些节点没有收到提交请求，导致部分参与者提交了事务，而有些参与者没有提交。&lt;/p&gt;
&lt;h2 id=&#34;分布式数据库对2pc的改进&#34;&gt;分布式数据库对2PC的改进
&lt;/h2&gt;&lt;p&gt;Percolator模型&lt;/p&gt;
&lt;p&gt;对于下面这个事务：“yes”转账100给“you”&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924232320.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;准备阶段，yes和you两边分别新增了一条记录，yes这里data减少100，you的data增加100；同时事务管理器记录下这次操作的日志；注意这里Lock部分有数据代表这是私有数据，别的事务读取不到。&lt;/li&gt;
&lt;li&gt;yes这里的Lock获得了一个标志PK，而you这里的Lock指向了PK，说明you这里记录的更新依赖于PK这条记录；&lt;/li&gt;
&lt;li&gt;提交阶段，事务管理器只会向PK这条记录发送提交请求，PK记录会将PK标志删除，此时该条记录变为公开版本，其他事务可见；&lt;/li&gt;
&lt;li&gt;下一次访问 PK@yes 记录时会发现PK记录已经提交，所以会将 PK@yes 也删去提交；注意这里实际上会有后台线程扫描进行此操作，因此不会真的等到下一次访问才提交，不会影响执行效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;改进点&#34;&gt;改进点
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;提交阶段只与一个参与者交互，这个操作是原子的，解决了&lt;strong&gt;数据不一致&lt;/strong&gt;问题；&lt;/li&gt;
&lt;li&gt;事务管理器会记录操作日志，在宕机后选举新的事务管理器可以通过日志继续工作，解决了&lt;strong&gt;单点故障&lt;/strong&gt;问题；&lt;/li&gt;
&lt;li&gt;后台线程扫描事务状况，在事务管理宕机后会回滚各个参与者的事务（2PC中参与者没有接收到协调者消息不会进行提交或回滚操作）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;xa规范-2pc落地实现&#34;&gt;XA规范-2PC落地实现
&lt;/h2&gt;&lt;p&gt;XA规范是基于2PC的，XA是对DTP（Distributed Transaction Processing）模型中的事务管理器（TM）和资源管理器（RM）之间的交互进行约束后形成的规范。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924232324.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;AP：应用程序，事务发起者&lt;/p&gt;
&lt;p&gt;RM：资源管理器，可以认为是数据库，对应上面2PC中的参与者&lt;/p&gt;
&lt;p&gt;TM：事务管理器，对应2PC中的协调者&lt;/p&gt;
&lt;p&gt;实际实现时可以由一个角色实现两个功能，比如AP实现TM的功能&lt;/p&gt;
&lt;h3 id=&#34;mysql-xa&#34;&gt;MySQL XA
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Client作为AP和TM的角色;&lt;/li&gt;
&lt;li&gt;准备阶段，Client向不同的数据库（RM）发送不同的指令；&lt;/li&gt;
&lt;li&gt;提交阶段，Client根据RM返回的情况决定执行commit还是rollback。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;3pc&#34;&gt;3PC
&lt;/h1&gt;&lt;p&gt;相较于2PC，多了一个预提交阶段&lt;/p&gt;
&lt;p&gt;3PC和2PC对应关系如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;准备（询问参与者是否状态正常） &amp;lt;&amp;mdash;&amp;gt; 无对应&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;预提交（发送指令） &amp;lt;&amp;mdash;&amp;gt; 准备&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;提交（通知提交） &amp;lt;&amp;mdash;&amp;gt; 提交&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924232328.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;引入准备阶段的目的就是为了解决2PC中由于个别参与者不正常导致其他参与者锁定资源，但异常情况发生几率较小，因此额外引入的阶段会在正常情况下带来额外的时间开销，不划算。&lt;/p&gt;
&lt;p&gt;3PC同时引入了超时机制，当预提交阶段过后，如果迟迟未收到协调者提交请求，参与者会&lt;strong&gt;自动提交&lt;/strong&gt;，因为在第一步准备阶段时就已经确定参与者是否都认可，因此进入预提交阶段说明所有参与者都可以执行并提交。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实际上3PC的设计是想解决2PC中带来的问题，但实际上并没有很好解决，反而性能更差了，3PC也没有落地实现。&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;tcc&#34;&gt;TCC
&lt;/h1&gt;&lt;p&gt;2PC和3PC都是针对数据库的事务提交和回滚，但很多情况下，业务操作不止局限于数据库，也可能是发送短信，上传图片等操作，TCC就是一种&lt;strong&gt;业务层面的两阶段提交&lt;/strong&gt;方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TCC - Try Confirm Cancel，分别指代业务层面要实现的3个方法&lt;/li&gt;
&lt;li&gt;第一阶段（Try），资源检查预留阶段；&lt;/li&gt;
&lt;li&gt;第二阶段（提交、回滚）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如对于一个扣款的服务，Try方法用于冻结扣款资金，Confirm用于执行真正的扣款，Cancel方法用于Try方法的回滚（资金解冻）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924232332.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;TCC对于业务有很大侵入，但是不会阻塞资源，每个方法都是直接提交事务的。&lt;/p&gt;
&lt;p&gt;如果部分Confirm失败，则会一直重试，直到成功，或者记录下来，人工处理。&lt;/p&gt;
&lt;h2 id=&#34;幂等性&#34;&gt;幂等性
&lt;/h2&gt;&lt;p&gt;Try、Confirm、Cancel三个方法需要保证幂等性，因为网络问题可能造成请求多次发送，需要幂等性保证不会因为多次执行出错。&lt;/p&gt;
&lt;h2 id=&#34;空回滚问题&#34;&gt;空回滚问题
&lt;/h2&gt;&lt;p&gt;Try方法由于网络问题超时而没有到达参与者，事务管理器会发出Cancel命令，如果此时收到Cancel，需要能够处理这种没有执行Try的情况下进行Cancel。&lt;/p&gt;
&lt;h2 id=&#34;悬挂问题&#34;&gt;悬挂问题
&lt;/h2&gt;&lt;p&gt;在空回滚问题执行了Cancel后此前超时的Try命令到达了；在空回滚之后需要记录一下，防止这种情况下Try的再次调用。&lt;/p&gt;
&lt;h1 id=&#34;tcc变体&#34;&gt;TCC变体
&lt;/h1&gt;&lt;h2 id=&#34;没有try的tcc&#34;&gt;没有Try的TCC
&lt;/h2&gt;&lt;p&gt;取消了Try操作，在一阶段直接执行Confirm操作，第二阶段判断成功则无操作，失败则执行Cancel&lt;/p&gt;
&lt;p&gt;其实和2PC很像：一阶段执行操作，二阶段可能回滚；只是2PC一阶段没有提交，这里一阶段提交了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924232337.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;异步tcc&#34;&gt;异步TCC
&lt;/h2&gt;&lt;p&gt;如果某些服务很难改造，且不会影响主业务决策，可以引入可靠消息服务，通过消息服务来代替个别服务进行Try、Confirm、Cancel。&lt;/p&gt;
&lt;p&gt;Try写入消息，Confirm发送消息，Cancel取消消息发送&lt;/p&gt;
&lt;h1 id=&#34;本地消息表&#34;&gt;本地消息表
&lt;/h1&gt;&lt;p&gt;这是基于最终一致性思想实现的一种方案。&lt;/p&gt;
&lt;p&gt;将&lt;strong&gt;业务执行&lt;/strong&gt;和&lt;strong&gt;消息插入&lt;/strong&gt;放在同一个本地事务中执行，然后执行下一个服务，如果调用成功则改变这条本地消息的状态，如果失败则会一直调用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/Messiz/image-bed/test/20240924232347.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;事务消息&#34;&gt;事务消息
&lt;/h1&gt;&lt;p&gt;RocketMQ提供事务消息，事务开始时发送&lt;strong&gt;半消息&lt;/strong&gt;（暂不能投递的消息，发送方已经将消息成功发送到了 MQ 服务端，但是服务端未收到生产者对该消息的二次确认，此时该消息被标记成“暂不能投递”状态，处于该种状态下的消息即半消息）给broker，此时消息对消费者不可见，当本地事务提交了，半事务也就提交，此时消费者可见，也就可以消费了。&lt;/p&gt;
&lt;p&gt;broker如果很久没有等到半消息提交或是回滚的消息，则会反查生产者，保证事务一致性。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
